# -*- coding: utf-8 -*-
"""Fake News Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BB0ZF4CHw2OFaYWDN8zUVx4Fh2ttR45B

A full training dataset with the following attributes:

id: unique id for a news article
title: the title of a news article
author: author of the news article
text: the text of the article; could be incomplete
label: a label that marks the article as potentially unreliable
1: unreliable
0: reliable
"""

# Importing the dependancies
import numpy as np
import pandas as pd
import re 
from nltk.corpus import stopwords # stopwords are the ones which doesnt add much context to our data
from nltk.stem.porter import PorterStemmer # this is used to stem the words (for finding the root words)
from sklearn.feature_extraction.text import TfidfVectorizer #to convert the text into feature vectors
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression # this is one of the best algo for predicting Binary classification data
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

print(stopwords.words('english')) #These words doesnt add any purpose for the data

"""Data Preprocessing

"""

#LOADING THE DATASET TO PANDAS DATAFRAME

news_dataset = pd.read_csv('/content/train.csv')
print(news_dataset)

news_dataset['label'].value_counts()

#Counting the number of missing values in the dataset

news_dataset.isnull().sum()

"""As the number of missing values are small in number when compared to the total dataset, we can drop them or replace them with NULL strings. If the numbers are higher then we use some methods like "Imputations" to fill the values with appropriate values

"""

#Replacing the null values with NULL strings or empty strings
news_dataset = news_dataset.fillna('')

#Combining the tite and author name columns for carrying out the further operations
news_dataset['content'] = news_dataset['author']+' '+news_dataset['title']

print(news_dataset['content'])

#Seperating the data and label ie., content and label columns
X = news_dataset.drop(columns='label', axis=1)
Y = news_dataset['label']

print(X)
print(Y)

"""Stemming:

Stemming is the process of reducing the word to its root word

Example:
actor,actress,acting,action --> For all these words the root word is act
"""

porter_stem = PorterStemmer()

def stemming(content):
  stemmed_content = re.sub('[^a-zA-Z]', ' ', content)
  stemmed_content = stemmed_content.lower()
  stemmed_content = stemmed_content.split()
  stemmed_content = [porter_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content = ' '.join(stemmed_content)
  return stemmed_content

news_dataset['content'] = news_dataset['content'].apply(stemming)

print(news_dataset['content'])

#Seperating the data and label columns
X = news_dataset['content'].values
Y = news_dataset['label'].values

print(X, Y)

# Converting the textual data to numerical data for the system to understand

vectorizer = TfidfVectorizer()  
vectorizer.fit(X)

X = vectorizer.transform(X)

print(X)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y,random_state=2)

"""Training the model"""

model = LogisticRegression()

model.fit(X_train, Y_train)

"""Evaluation of the model with training dataset"""

X_train_prediction = model.predict(X_train)
training_data_accuracy_score = accuracy_score(X_train_prediction, Y_train)
print('The training data accuracy is: ', training_data_accuracy_score)

X_test_prediction = model.predict(X_test)
test_data_accuracy_score = accuracy_score(X_test_prediction, Y_test)
print('The accuracy on test data is: ', test_data_accuracy_score)

"""MAking the predictive system"""

X_new = X_test[1]

prediction = model.predict(X_new)
print(prediction)

if (prediction[0] == 0):
  print('This is a real news!')
else:
  print('This is a fake news!!!')

print(Y_test[1])

